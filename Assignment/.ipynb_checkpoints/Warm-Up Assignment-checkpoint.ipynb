{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks (RNNs)\n",
    "## An intro to sequence classification, and an aside on LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you get started on this assignment, please make sure to look at the note in the Teaching folder in the repository to review the concepts behind RNN architectures. This initial toy example will help you practice building RNNs and LSTMs -- much of the work required in the main project will build off of this initial warm up assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import backend as K\n",
    "from keras.layers import Bidirectional, SimpleRNN, LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs) - An intro to sequence classification, and an aside on LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you started, we look at a simple example built specifically for RNNs - digit sequence classification. We define the problem as follows: assume you are an airline company running baggage checks at the airport. The maximum total baggaged weight allowed for the flight is $1570$ pounds. We classify a baggage weight with label $0$ if the cumulative sum so far is lesser than the threshold, and $1$ if it goes above the threshold. Also assume that the length of all sequences is $35$. For example:\n",
    "\n",
    "Sequence: $500, 500, 500, 327, 294, 102, \\ldots$  \n",
    "Labels  : $0  , 0  , 0  , 1  , 1  , 1, \\ldots$\n",
    "\n",
    "We will also plot confusion matrices and accuracy plots to inform our decisions about the model. Here's a quick refresher/introduction to confusion matrices: A confusion matrix is a way to visualize classifier performance (both binary and multiclass classifiers) - specifically, the matrix captures the number of True Positives (predicted 1 and true label 1), True Negatives (predicted 0 and true label 0), False Positives (predicted 1 but true label 0) and False Negatives (predicted 0 but true label 1). \n",
    "\n",
    "You can also refer to this link for more information: https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/#:~:text=A%20confusion%20matrix%20is%20a,related%20terminology%20can%20be%20confusing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sequence data (data matrix $X$ and label matrix $y$) using the functions defined below and then perform a 80-20 train-test split to generate training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baggage_threshold = 1570\n",
    "sequence_len = 35\n",
    "\n",
    "def generate_sequence(sequence_len):\n",
    "    data = np.random.choice(521, sequence_len) #generate random ints in the given range\n",
    "    labels = np.array([0 if np.sum(data[: i+1]) <= 1570 else 1 for i in range(len(data))])\n",
    "    \n",
    "    return data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct data matrix X and labels y\n",
    "\n",
    "X = np.zeros((1000, 35))\n",
    "y = np.zeros((1000, 35))\n",
    "\n",
    "#TODO: your code here\n",
    "    \n",
    "#split into training and testing data using an 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train a simple RNN to classify our data! We will be using binary cross-entropy loss, which you will see later in this notebook as well.  Here is some starter code to help you out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(SimpleRNN(20, input_shape = (35, 1), return_sequences = True))\n",
    "\n",
    "#add an output layer (Hint: Look up sigmoid activation)\n",
    "#TODO: your code here\n",
    "\n",
    "#compile your model with a binary cross-entropy loss and fit it using X_train and y_train\n",
    "#TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check our model accuracy on the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#evaluate your model on the test set and report the test loss and accuracy\n",
    "#TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can better illustrate our prediction performance using a confusion matrix. Recall that a confusion matrix is a matrix in which each row represents an actual label and each column represents a predicted label such that the value in the row for label $i$ and column for label $j$ represents the number of datapoints with actual label $i$ for which the model predicted label $j$. So in our binary classification case, there are 2 possible labels -- $0$ and $1$, so the off-diagonal elements of the confusion matrix tell us how many $x$s with actual label $0$ were predicted $1$ and how many $x$s with actual label $1$ were predicted $0$ while the diagonal elements tell us how many $x$s were correctly predicted, for actual label $0$ and $1$.\n",
    "\n",
    "We plot the confusion matrix for this model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten predictions tensor into a list\n",
    "pred = list(itertools.chain(*model_1.predict_classes(X_test.reshape(200, 35, 1))))\n",
    "pred = [i[0] for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten y_test values into a list\n",
    "actual = []\n",
    "for row in range(len(y_test)): \n",
    "    actual += list(y_test[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#horizontal axis is predicted label, vertical axis is actual label\n",
    "conf_mat = confusion_matrix(actual, pred, labels = [0, 1])\n",
    "df_cm = pd.DataFrame(conf_mat)\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#code to plot the confusion matrix\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "ax = sn.heatmap(df_cm, cmap=\"BuPu\")\n",
    "ax.set(xlabel='Predicted Label', ylabel='Actual Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Although we have a high overall accuracy, the confusion matrix shows that there are many terms with an actual label of $0$ that are classifier predicts to be $1$. Let's see if we can do better using LSTMs (a form of RNNs, refer to the note for more details) - make a new model that uses an LSTM layer instead of a simple RNN. Notice that it makes sense to use LSTMs here because we would like our model to remember earlier baggage weights to classify future baggage weights in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "#add an LSTM layer and a dense layer -- use 20 units for the LSTM (same as for the SimpleRNN)\n",
    "#TODO: your code here\n",
    "\n",
    "#compile your model with a binary cross-entropy loss and fit it using X_train and y_train\n",
    "#TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check our model accuracy on the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#evaluate your model on the test set and report the test loss and accuracy\n",
    "#TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again plot the confusion matrix for this model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten predictions tensor into a list\n",
    "pred = list(itertools.chain(*model_2.predict_classes(X_test.reshape(200, 35, 1))))\n",
    "pred = [i[0] for i in pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate confusion matrix\n",
    "conf_mat = confusion_matrix(actual, pred, labels = [0, 1])\n",
    "df_cm = pd.DataFrame(conf_mat, range(2), range(2))\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#code to plot the confusion matrix\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "ax = sn.heatmap(df_cm, cmap=\"BuPu\")\n",
    "ax.set(xlabel='Predicted Label', ylabel='Actual Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better! As you can see from the confusion matrix, there are fewer false positives, i.e. terms that are predicted to be under label $1$ but are actually under label $0$. Now let's try using a Bidirectional LSTM as our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "#add a Bidirectional LSTM layer and a dense layer -- use 20 units for the Bi-LSTM (same as for the SimpleRNN)\n",
    "#TODO: your code here\n",
    "\n",
    "#compile your model with a binary cross-entropy loss and fit it using X_train and y_train\n",
    "#TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check our model accuracy on the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#evaluate your model on the test set and report the test loss and accuracy\n",
    "#TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again plot the confusion matrix for this mode below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(itertools.chain(*model_3.predict_classes(X_test.reshape(200, 35, 1))))\n",
    "pred = [i[0] for i in pred]\n",
    "\n",
    "#generate confusion matrix\n",
    "conf_mat = confusion_matrix(actual, pred, labels = [0, 1])\n",
    "df_cm = pd.DataFrame(conf_mat, range(2), range(2))\n",
    "df_cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to plot the confusion matrix\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "ax = sn.heatmap(df_cm, cmap=\"BuPu\")\n",
    "ax.set(xlabel='Predicted Label', ylabel='Actual Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, generate a plot of training set accuracy and loss as a function of the number of epochs that our model has trained for. This shows us how much better our training set accuracy improves with each additional epoch of training. To better understand whether or not we are overfitting after a certain number of epochs, we would have to include a validation set. We have not done that here but feel free to play around with generating a validation set from our data and also including plots for validation accuracy and loss to determine the optimal number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "    \n",
    "train_losses_1 = #TODO: fill in\n",
    "train_accuracies_1 = #TODO: fill in\n",
    "\n",
    "train_losses_2 = #TODO: fill in\n",
    "train_accuracies_2 = #TODO: fill in\n",
    "\n",
    "train_losses_3 = #TODO: fill in\n",
    "train_accuracies_3 = #TODO: fill in\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Accuracies vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(train_accuracies_1, label=\"RNN Accuracy\")\n",
    "plt.plot(train_accuracies_2, label=\"LSTM Accuracy\")\n",
    "plt.plot(train_accuracies_3, label=\"Bi-LSTM Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Losses vs Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.plot(train_losses_1, label=\"RNN Losses\")\n",
    "plt.plot(train_losses_2, label=\"LSTM Losses\")\n",
    "plt.plot(train_losses_3, label=\"Bi-LSTM Losses\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that in the Bidirectional case, you may have either gotten a marginal increase in accuracy or a slight decrease in accuracy, depending on how your sequences were generated. While this example was specifically desgined to make RNNs and LSTMs shine, we hope this gives you an appreciation for the kinds of problems we can solve using Recurrent Networks, and how improvements can be made to the simple RNN architecture. Next, in the main project, we will try to use RNNs to tackle a much harder problem - stock price classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to Develop a Bidirectional LSTM For Sequence Classification in Python with Keras: <br> https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
