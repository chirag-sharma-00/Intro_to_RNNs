{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up 1 - Building an RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies. We'll only be using numpy for this part\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you'll be building the forward pass for a simple RNN layer from scratch, using concepts you've seen in the note and the slides. We've implemented the Backpropagation Through Time for you, but we'd recommend going through the code and trying to understand what exactly it's doing. Be sure to review the RNN architecture and relevant equations from the note before getting started:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an `RNN` class with various functions below. \n",
    "- To initialize the RNN model, first randomly initialize the 3 weight matrices $\\mathbf{W}_a, \\mathbf{W}_x, \\mathbf{W}_y$. \n",
    "- Code a vectorized softmax activation that you will need to use in your forward pass algorithm. Remember that for $\\mathbf{x} = \\begin{bmatrix} x_1 & \\cdots & x_n \\end{bmatrix}^T$, $softmax(\\mathbf{x}) = \\begin{bmatrix} \\displaystyle \\frac{e^{x_1}}{\\sum_{i = 1}^n e^{x_i}} & \\cdots & \\displaystyle \\frac{e^{x_n}}{\\sum_{i = 1}^n e^{x_i}} \\end{bmatrix}^T$.\n",
    "- Then, code the forward pass algorithm using the equations $\\mathbf{a}^{\\langle t \\rangle} = \\tanh(\\mathbf{W}_a \\mathbf{a}^{\\langle t - 1 \\rangle} + \\mathbf{W}_x \\mathbf{x}^{\\langle t \\rangle})$ and $\\mathbf{\\hat y}^{\\langle t \\rangle} = \\tanh(\\mathbf{W}_y \\mathbf{a}^{\\langle t \\rangle})$. We set $\\mathbf{a}^{\\langle 0 \\rangle}$ to $\\mathbf{0}$ for this.\n",
    "- The backpropagation algorithm and the prediction function have been implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(): \n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = 10): \n",
    "        #Initialize the three weight matrices here, use np.random.rand and normalize by a factor of 1000\n",
    "        #TODO:\n",
    "        self.Wa = ...\n",
    "        self.Wx = ...\n",
    "        self.Wy = ...\n",
    "        \n",
    "        #keep track of current input to model\n",
    "        self.input = np.zeros(input_dim)\n",
    "        \n",
    "        #keep track of current output at each time step\n",
    "        self.output = {}\n",
    "        for key in range(input_dim + 1):\n",
    "            self.output[key] = np.zeros(hidden_dim)\n",
    "        \n",
    "    #TODO: implement softmax activation using numpy\n",
    "    def softmax(self, x): \n",
    "        return ...\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        #Code the forward pass for an RNN\n",
    "        #TODO: store the current input to the model in self.input\n",
    "        self.input = ...\n",
    "        \n",
    "        #TODO: forward steps for the RNN: compute a_t for each t and update the relevant entry in self.output.    \n",
    "            \n",
    "        #compute y_t and a_t for the last timestep\n",
    "        t_final = ...\n",
    "        a_t = ...\n",
    "        y_t = ...\n",
    "            \n",
    "        return y_t, a_t\n",
    "    \n",
    "    \n",
    "    #Backprop Through Time using Cross Entropy loss function\n",
    "    def backward_pass(self, dy): \n",
    "\n",
    "        n = len(self.input)\n",
    "        dWy = np.outer(dy, self.output[n])\n",
    "\n",
    "        dWa= np.zeros(self.Wa.shape)\n",
    "        dWx = np.zeros(self.Wx.shape)\n",
    "        d_a = self.Wy.T @ dy\n",
    "\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(n)):\n",
    "            temp = ((1 - self.output[t + 1] ** 2) * d_a)\n",
    "            dWa += temp @ self.output[t + 1].T\n",
    "            dWx += np.outer(temp, self.input[t].T)\n",
    "            d_a = self.Wa @ temp\n",
    "        \n",
    "        return dWa, dWx, dWy\n",
    "    \n",
    "    def update_weights(self, dWa, dWx, dWy, learn_rate):\n",
    "        self.Wa -= learn_rate * dWa\n",
    "        self.Wx -= learn_rate * dWx\n",
    "        self.Wy -= learn_rate * dWy\n",
    "        \n",
    "    def classify(self, y_vec): \n",
    "        for i in range(len(y_vec)): \n",
    "            if y_vec[i] > 0.5: \n",
    "                preds = 1\n",
    "            else: \n",
    "                preds = 0          \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "Let's construct a problem to solve using our very own RNN. Consider this very simple toy example - you receive multiple sequences of baggage weights - each sequence has 2 pieces of luggage, with a max weight of 40 pounds, and you have 25 passengers each with their own baggage sequence. The total weight your flight can carry is 1000 pounds. You have a simple task - see if your flight can handle the input baggage weights or not. Note that you could essentially sum each sequence and see if the total weight is > 1000 - neural networks are definitely overkill in this scenario, but let's say the airport is inefficient really wants to use more computational resources than required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generation functions\n",
    "def generate_sequence(input_dims):\n",
    "    #generate matrix of shape input_dims containing random ints in the range [0, 40)\n",
    "    data = np.random.choice(40, input_dims)\n",
    "    total_sum = sum([sum(arr) for arr in data])\n",
    "    label = 1 if total_sum > 1000 else 0\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first generate 1000 samples\n",
    "X = [0] * 1000\n",
    "y = np.zeros(1000)\n",
    "\n",
    "#TODO: use the generate_sequence function to generate X and y for the training dataset\n",
    "#We want each sample to be of shape (25, 2), i.e., a sequence of 2 25-dimensional vectors\n",
    "for idx in range(1000): \n",
    "    x_gen, y_gen = ...\n",
    "    X[idx] = ...\n",
    "    y[idx] = ...\n",
    "    \n",
    "#split into training and testing data using an 80/20 split\n",
    "X_train = X[:800]\n",
    "y_train = y[:800]\n",
    "X_test = X[800:]\n",
    "y_test = y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: initialize an RNN with output dim 2, we want to see the probability of each class. Use 10 for hidden dim\n",
    "rnn = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RNN for 10 epochs\n",
    "for i in range (10): \n",
    "    for j in range(len(X_train)): \n",
    "        x = X_train[j]\n",
    "        y = y_train[j]\n",
    "        \n",
    "        class_probs = rnn.forward_pass(x)[0]\n",
    "        class_pred = rnn.classify(class_probs)\n",
    "            \n",
    "        dy = class_probs\n",
    "        dy[int(y)] -= 1\n",
    "                \n",
    "        #TODO: run the backprop for each iteration\n",
    "        derivatives = ...\n",
    "        rnn.update_weights(...)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test our simple RNN on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct_test = 0\n",
    "for j in range(len(X_test)): \n",
    "    x = X_test[j]\n",
    "    y = y_test[j]\n",
    "\n",
    "    class_probs = rnn.forward_pass(x)[0]\n",
    "    class_pred = rnn.classify(class_probs)\n",
    " \n",
    "    if class_pred == y: \n",
    "        num_correct_test += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate your testing accuracy below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 1: How did our RNN perform? (Optional question): How would we change the backward pass function if we were using MSE loss instead of cross entropy loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "- An Introduction to RNNs <br>\n",
    "  https://victorzhou.com/blog/intro-to-rnns/\n",
    "  \n",
    "- Backpropagation through time <br>\n",
    "  http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
