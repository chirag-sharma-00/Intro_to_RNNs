{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up 1 - Building an RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you'll be building the forward pass for a simple RNN layer from scratch, using concepts you've seen in the note and the slides. We've implemented the Backpropagation Through Time for you, but we'd recommend going through the code and trying to understand what exactly it's doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(): \n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = 10): \n",
    "        #Initialize the three weight matrices here, use np.random.rand and normalize by a factor of 1000\n",
    "        #TODO:\n",
    "        self.Wa = ...\n",
    "        self.Wx = ...\n",
    "        self.Wy = ...\n",
    "        \n",
    "        #keep track of previous inputs\n",
    "        self.prev_inps = 0\n",
    "        \n",
    "    #TODO: implement softmax activation using numpy\n",
    "    def softmax(self, x): \n",
    "        return \n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        #Code the forward pass for an RNN\n",
    "        #TODO: \n",
    "        \n",
    "        #initialize a, i.e. hidden state\n",
    "        a = ...\n",
    "        \n",
    "        self.prev_inps = inputs \n",
    "        self.prev_a_s = {0 : a}\n",
    "        \n",
    "        #forward steps for the RNN, compute a_t and y_t. Refer to the note if you're stuck!\n",
    "        for key, input_vec in enumerate(inputs): \n",
    "            a_key = ...\n",
    "            self.prev_a_s[key + 1] = ...\n",
    "            \n",
    "            \n",
    "        #calculate y and apply softmax\n",
    "        y = ...\n",
    "            \n",
    "        return y, a_key\n",
    "    \n",
    "    \n",
    "    #Backprop Through Time using Cross Entropy loss function\n",
    "    def backward_pass(self, dy): \n",
    "\n",
    "        n = len(self.prev_inps)\n",
    "        \n",
    "        dWy = np.outer(dy, self.prev_a_s[n])\n",
    "        dWa= np.zeros(self.Wa.shape)\n",
    "        dWx = np.zeros(self.Wx.shape)\n",
    "\n",
    "        d_a = self.Wy.T @ dy\n",
    "\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(n)):\n",
    "            temp = ((1 - self.prev_a_s[t + 1] ** 2) * d_a)\n",
    "            dWa += temp @ self.prev_a_s[t + 1].T\n",
    "            dWx += np.outer(temp, self.prev_inps[t].T)\n",
    "            d_a = self.Wa @ temp\n",
    "        \n",
    "        return dWa, dWx, dWy\n",
    "    \n",
    "    def update_weights(self, dWa, dWx, dWy, learn_rate):\n",
    "        self.Wa -= learn_rate * dWa\n",
    "        self.Wx -= learn_rate * dWx\n",
    "        self.Wy -= learn_rate * dWy\n",
    "        \n",
    "    def classify(self, y_vec): \n",
    "        \n",
    "        for i in range(len(y_vec)): \n",
    "            if y_vec[i] > 0.5: \n",
    "                preds = 1\n",
    "            else: \n",
    "                preds = 0\n",
    "                \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "Let's construct a problem to solve using our very own RNN. Consider this very simple toy example - you receive multiple sequences of baggage weights - each sequence has 5 pieces of luggage, with a max weight of 40 pounds, and you have 100 passengers each with their own baggage sequence. The total weight your flight can carry is 1570 pounds. You have a simple task - see if your flight can handle the input baggage weights or not. Note that you could essentially sum each sequence and see if the total weight is > 1570 - neural networks are definitely overkill in this scenario, but let's say the airport is inefficient really wants to use more computational resources than required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generation functions\n",
    "def generate_sequence(input_dims):\n",
    "    #input dims is the (100, 5)\n",
    "    data = np.random.choice(40, input_dims) #generate random ints in the given range\n",
    "    total_sum = sum([sum(arr) for arr in data])\n",
    "    label = 1 if total_sum > 1570 else 0\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Generate 1000 samples\n",
    "X = [0] * 1000\n",
    "y = np.zeros(1000)\n",
    "\n",
    "for row in range(1000): \n",
    "    x_gen, y_gen = ...\n",
    "    X[row] = ...\n",
    "    y[row] = ...\n",
    "    \n",
    "#split into training and testing data using an 80/20 split\n",
    "X_train = X[:800]\n",
    "y_train = y[:800]\n",
    "X_test = X[800:]\n",
    "y_test = y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize an RNN with output dim 2, we want to see the probability of each class\n",
    "rnn = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RNN for 10 epochs\n",
    "for i in range (10): \n",
    "    for j in range(len(X_train)): \n",
    "        x = X_train[j]\n",
    "        y = y_train[j]\n",
    "        \n",
    "        class_probs = rnn.forward_pass(x)[0]\n",
    "        class_pred = rnn.classify(class_probs)\n",
    "            \n",
    "        dy = class_probs\n",
    "        dy[int(y)] -= 1\n",
    "                \n",
    "        #TODO: run the backprop for each iteration\n",
    "        derivatives = ...\n",
    "        rnn.update_weights()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test our simple RNN on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct_test = 0\n",
    "for j in range(len(X_test)): \n",
    "    x = X_test[j]\n",
    "    y = y_test[j]\n",
    "\n",
    "    class_probs = rnn.forward_pass(x)[0]\n",
    "    class_pred = rnn.classify(class_probs)\n",
    " \n",
    "    if class_pred == y: \n",
    "        num_correct_test += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate your testing accuracy below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our RNN perfomed really well on this simple task. Take some time to go through the backpropagation part of the RNN if you haven't already. Here's a challenge: how would our updates change id we used MSE loss instead of cross-entropy loss? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "- An Introduction to RNNs <br>\n",
    "  https://victorzhou.com/blog/intro-to-rnns/\n",
    "  \n",
    "- Backpropagation through time <br>\n",
    "  http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
