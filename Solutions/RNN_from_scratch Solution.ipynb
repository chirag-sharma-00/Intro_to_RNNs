{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up 1 - Building an RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dependencies. We'll only be using numpy for this part!\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you'll be building the forward pass for a simple RNN layer from scratch, using concepts you've seen in the note and the slides. We've implemented the Backpropagation Through Time for you, but we'd recommend going through the code and trying to understand what exactly it's doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(): \n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim = 10): \n",
    "        #Initialize the three weight matrices here, use np.random.rand and normalize by a factor of 1000\n",
    "        #TODO:\n",
    "        self.Wa = np.random.rand(hidden_dim, hidden_dim)/1000\n",
    "        self.Wx = np.random.rand(hidden_dim, input_dim)/1000\n",
    "        self.Wy = np.random.rand(output_dim, hidden_dim)/1000\n",
    "        \n",
    "        #keep track of previous inputs\n",
    "        self.prev_inps = 0\n",
    "        \n",
    "    #TODO: implement softmax activation using numpy\n",
    "    def softmax(self, x): \n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "    \n",
    "    def forward_pass(self, inputs):\n",
    "        #Code the forward pass for an RNN\n",
    "        #TODO: \n",
    "        a = np.zeros(self.Wa.shape[0])\n",
    "        \n",
    "        self.prev_inps = inputs \n",
    "        self.prev_a_s = {0 : a}\n",
    "        \n",
    "        #forward steps for the RNN, compute a_t and y_t \n",
    "        for key, input_vec in enumerate(inputs): \n",
    "            a_key = np.tanh(self.Wa@a + self.Wx@input_vec)\n",
    "            self.prev_a_s[key + 1] = a_key\n",
    "        \n",
    "        y = self.softmax(self.Wy@a_key)\n",
    "            \n",
    "        return y, a_key\n",
    "    \n",
    "    \n",
    "    #Backprop Through Time using Cross Entropy loss function\n",
    "    def backward_pass(self, dy): \n",
    "\n",
    "        n = len(self.prev_inps)\n",
    "        dWy = np.outer(dy, self.prev_a_s[n])\n",
    "\n",
    "        dWa= np.zeros(self.Wa.shape)\n",
    "        dWx = np.zeros(self.Wx.shape)\n",
    "        d_a = self.Wy.T @ dy\n",
    "\n",
    "        # Backpropagate through time\n",
    "        for t in reversed(range(n)):\n",
    "            temp = ((1 - self.prev_a_s[t + 1] ** 2) * d_a)\n",
    "            dWa += temp @ self.prev_a_s[t + 1].T\n",
    "            dWx += np.outer(temp, self.prev_inps[t].T)\n",
    "            d_a = self.Wa @ temp\n",
    "        \n",
    "        return dWa, dWx, dWy\n",
    "    \n",
    "    def update_weights(self, dWa, dWx, dWy, learn_rate):\n",
    "        self.Wa -= learn_rate * dWa\n",
    "        self.Wx -= learn_rate * dWx\n",
    "        self.Wy -= learn_rate * dWy\n",
    "        \n",
    "    def classify(self, y_vec): \n",
    "        \n",
    "        for i in range(len(y_vec)): \n",
    "            if y_vec[i] > 0.5: \n",
    "                preds = 1\n",
    "            else: \n",
    "                preds = 0\n",
    "                \n",
    "        return preds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "Let's construct a problem to solve using our very own RNN. Consider this very simple toy example - you receive multiple sequences of baggage weights - each sequence has 5 pieces of luggage, with a max weight of 40 pounds, and you have 100 passengers each with their own baggage sequence. The total weight your flight can carry is 1570 pounds. You have a simple task - see if your flight can handle the input baggage weights or not. Note that you could essentially sum each sequence and see if the total weight is > 1570 - neural networks are definitely overkill in this scenario, but let's say the airport is inefficient really wants to use more computational resources than required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generation functions\n",
    "def generate_sequence(input_dims):\n",
    "    #input dims is the (100, 5)\n",
    "    data = np.random.choice(40, input_dims) #generate random ints in the given range\n",
    "    total_sum = sum([sum(arr) for arr in data])\n",
    "    label = 1 if total_sum > 1570 else 0\n",
    "    \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First generate 1000 samples\n",
    "X = [0] * 1000\n",
    "y = np.zeros(1000)\n",
    "\n",
    "for row in range(1000): \n",
    "    x_gen, y_gen = generate_sequence((100, 5))\n",
    "    X[row] = x_gen\n",
    "    y[row] = y_gen\n",
    "    \n",
    "#split into training and testing data using an 80/20 split\n",
    "X_train = X[:800]\n",
    "y_train = y[:800]\n",
    "X_test = X[800:]\n",
    "y_test = y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize an RNN with output dim 2, we want to see the probability of each class\n",
    "rnn = RNN(5, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train RNN for 10 epochs\n",
    "for i in range (10): \n",
    "    for j in range(len(X_train)): \n",
    "        x = X_train[j]\n",
    "        y = y_train[j]\n",
    "        \n",
    "        class_probs = rnn.forward_pass(x)[0]\n",
    "        class_pred = rnn.classify(class_probs)\n",
    "            \n",
    "        dy = class_probs\n",
    "        dy[int(y)] -= 1\n",
    "                \n",
    "        derivatives = rnn.backward_pass(dy)\n",
    "        rnn.update_weights(derivatives[0], derivatives[1], derivatives[2], 0.05)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test our simple RNN on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct_test = 0\n",
    "for j in range(len(X_test)): \n",
    "    x = X_test[j]\n",
    "    y = y_test[j]\n",
    "\n",
    "    class_probs = rnn.forward_pass(x)[0]\n",
    "    class_pred = rnn.classify(class_probs)\n",
    " \n",
    "    if class_pred == y: \n",
    "        num_correct_test += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate your testing accuracy below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct_test/len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion Question 1: How did our RNN perform? (Optional question): How would we change the backward pass function if we were using MSE loss instead of cross entropy loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here:** Great! Our RNN perfomed really well on this simple task. Take some time to go through the backpropagation part of the RNN if you haven't already. If we were to use the MSE loss instead of cross entropy loss, we would have to re-calculate the partial derivatives w.r.t to the weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References: \n",
    "- An Introduction to RNNs <br>\n",
    "  https://victorzhou.com/blog/intro-to-rnns/\n",
    "  \n",
    "- Backpropagation through time <br>\n",
    "  http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
